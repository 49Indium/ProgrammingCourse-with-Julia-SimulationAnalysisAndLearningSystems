# Unit 7: Working with heterogenous datasets and a view towards machine learning

In this unit we learn how to deal with heterogenous datasets using data frames. We then explore basic concepts of machine learning. A deep dive into deep learning at UQ is in the [STAT3007 course](https://my.uq.edu.au/programs-courses/course.html?course_code=STAT3007). Theory of machine learning (and statistical learning) is also studied in [STAT3006](https://my.uq.edu.au/programs-courses/course.html?course_code=STAT3006). Other aspects of applied statistics and data analysis are studied in [STAT3500](https://my.uq.edu.au/programs-courses/course.html?course_code=STAT3500). Our purpose with this unit is just to touch the tip of the iceberg on issues of data analysis and machine learning. The Julia language knowledge acquired in the previous units should help.

# Databases

This course doesn't touch databases, a rich topic of its own. (Do not confuse the term database with data structures or data frames). A database is a system that stores information in an organized and flexible manner. Many databases are **relational databases** and this means that they are comprised of multiple data tables that are associated via relations. The most common language for dealing with such databases is [SQL](https://en.wikipedia.org/wiki/SQL). It is not a general programming language but rather a language for querying, modifying, and managing databases. At UQ you can study more about databases in the [INFS2200 course](https://my.uq.edu.au/programs-courses/course.html?course_code=infs2200) as well as several other more advanced courses. 

We now show you an example of database and a few things you may expect from SQL.

## Relations

A "relation" is a mathematical term for a set over _tuples_, and is a generalization of a function.

For example, consider the function $y = f(x)$. An equivalent relation $f$ can be constructed as a set of tuples of the form $(x, y)$. A particular tuple $(x, y)$ is in the relation $f$ if an only if $y = f(x)$.

Other things which are not strictly functions may be relations. For example, we can construct a relation of real numbers $(x, y)$ where $x = y^2$. Or, equivalently, $y = \pm\sqrt{x}$. Positive values of $x$ correspond to two values of $y$ (positive or negative square root). When $x$ equals zero, there is only one solution for $y$ (also zero). And for negative $x$ there are no real $y$ that satisfy the relation. Note that I say "relation" and not "function" here - functions are only a subset of relations.

Just like functions may have multiple inputs ($x = f(a, b, c)$), a relation may consist of tuples of any size (including zero!). And both functions and relations can exist over finite, discrete sets. There are precisely two relations over tuples of size zero (i.e. $()$) - the empty relation, and the relation containing the empty tuple $()$. You can use this fact to build up a kind of boolean logic in the language of relations, where the empty relation is `false` and the relation containing $()$ is `true`.

Outside of mathematics, the kinds of relations normally modelled in databases are of the discrete, finite-sized variety. A relation over tuples $(a, b, c)$ is equivalent to table with three columns $a$, $b$ and $c$. In a _strictly_ relational databases, the ordering of the rows is irrelevant, and you cannot have two identical rows in the database (though in practice many databases in use may also deal with "bags" rather than "sets" of tuples, where rows can be repeated). The fields $a$, $b$ and $c$ are drawn from their own set of possibilities, or _datatype_. For example, you might have `(name, birthdate, is_adult)` where `name` may be a string, `birthdate` may be a date, and `is_adult` may be a boolean.

| `name` | `birthdate` | `is_adult` |
|-|-|-|
| `"Alice"` | `1999-05-13` | `true` |
| `"Bob"` | `2001-12-10` | `false` |
| `"Charlie"` | `2000-09-21` | `true` |

Note that this particular relation is also a function, since `is_adult` has a functional relationship with `birthdate`. In many relations, there are uniqueness constraints - for example the `name` might identify each row uniquely, or there may be a specific `ID` column to deal with different people sharing the same name. Such unique constraints are called "keys" and the primary identifier for a row is called the "primary key".

## Relational schema

A single relation is a restrictive way to model your data. However, by allowing multiple relations, we can capture the various sets of data in the system, and piece them together by their _relationships_. 

For example, each person in the table above might be related to other things. They might have jobs, bank accounts, etc. Within a single business a person might be a customer, a supplier and an employee, and for different purposes we might want data associated with that person (e.g. their birthdate might relate to promotions for customers, payrates for employees, etc).

Generally, to make keeping track of data easy, in a relational database you would store that person _just once_ and create relationships between that person and other things. That way, if something changes about that person, it only needs to be updated in one place, and data remains consistent everywhere.

Here is an example of a more complex example - an imagining of how _LinkedIn_ might store their data in a relational database.

![From _Designing Data-Intensive Applications_ by Martin Kleppmann](../web_img/schema.png)

## SQL

The most popular way to interact with relational data is with via a language called SQL (originally "SEQUEL", pronounced that way, backronymed to "structured query language").

The language is from 1973 and doesn't look like most other programming languages. With it, you declare a "query" and the database will find the best way to return the results. It is a form of "declaritive programming". Here is an example of getting some columns of data from the `users` table:

```sql
SELECT user_id, first_name, last_name
FROM users
```

You can filter the rows via a `WHERE` clause. Let's say you knew the `user_id` for Bill Gates and you wanted just his data:

```sql
SELECT user_id, first_name, last_name
FROM users
WHERE user_id = 251
```

Data in different tables are related via the `JOIN` statement

```sql
SELECT users.user_id, first_name, last_name, organization
INNER JOIN positions ON positions.user_id = users.user_id
FROM users
```

Note we have to prefix the shared column names with the corresponding table (although in this case the difference is not particularly important, your query cannot be ambiguous).

But there is a problem here. This query would create a large amount of data, that the database would need to collect and send over the network to you. In the worst case, performing such a query could even bring down a large system!

Generally, you have something more precise in mind. Like - which organizations did Bill Gates work at?

```sql
SELECT users.user_id, first_name, last_name, organization
INNER JOIN positions ON positions.user_id = users.user_id
FROM users
WHERE users.user_id = 251
```

This will now return just two rows (for Microsoft, and the Bill & Melinda Gates Foundation), and is much less effort for the database, the network and the client.

We won't be using SQL in this course but it is so pervasive in industry that is essential that you know that it exists, and not to be afraid to use it! (It is a very useful and desirable skill!).

# Dataframes

Working with tabular data is a central part of data analysis (think Excel). Data has rows (observations) and columns (variables/features). Typically a row would be for an individual, or item, or event. Typically a column would be for attributes of the individual, properties of the events, etc. Variables can be numerical, categorical, strings, or even more complex entities (e.g. images).

The datafile `athlete_events.csv` is a moderately sized dataset available from [Kaggle](https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results) covering all athletes that have participated the Summer or Winter Olympics between 1896 and 2016. The CSV file contains 40MB of data over 15 columns and more than 27,000 rows, which is "small data" for a modern computer (but would have been challenging in the 1980's). In this context, "big data" is too large to be handled by a single machine, and we'll discuss "big data" and "medium data" later.

You have already seen how to read and write from CSV files in Unit 3 where you also explored JSON files. You may recall using the `DataFrames.jl` package in that unit. We now take a deeper dive into the concept of dataframes.

We will use `athlete_events.csv` to show basic functionality of the [DataFrames.jl](https://dataframes.juliadata.org/stable/) and [TypedTables.jl](https://github.com/JuliaData/TypedTables.jl) packages. In a sense these packages are alternatives. Seeing some functionality from each may be useful for gaining a general understanding of what to expect from such packages. If you work with Python, then using [pandas](https://pandas.pydata.org/) is the common alternative. Similarly if you work with the R language then the built-in dataframes are common.

## Basic data exploration

Whenever you receive a dataset, it takes some time to understand its contents. This process is generally known as "data exploration". To do so, we generally load (a subset of) the data from disk and do some basic analysis in a Jupyter notebook or at the REPL.

```julia
using DataFrames, CSV
csv_file = CSV.File("../data/athlete_events.csv"; missingstring = "NA")
df = DataFrame(csv_file)
println("Data size: ", size(df))
println("Columns in the data: ", names(df))
```

Since the data is large, we don't usually want to print it all. The `first` and `last` functions can be helpful:

```julia
first(df, 10)
```

```julia
last(df, 10)
```

This dataset appears to be sorted by last name. Each row corresponds to an entrant to an event. The ID column identifies each unique athlete.

A `DataFrame` is indexed like a 2D container of rows and columns. The `:` symbol means "all".

```julia
df[1:10, :]
```

Internally, a dataframe stores one vector of data per column. You can access any of them conveniently as "properties" with `.` syntax:

```julia
df.ID
```

```julia
df.Height
```

As you can see from above, some data can be missing. How many heights are missing?

```julia
ismissing.(df.Height) |> count # Piping: `f(x) |> g` syntax means `g(f(x))`
```

The `count` function counts the number of `true` values.

How many unique athletes are there?

```julia
unique(df.ID) |> length
```

The `unique` operation has extra methods defined for `DataFrame`s. We can get the athlete data like so:

```julia
athlete_df = unique(df, :ID)
```

It keeps all columns of the first row containing each distinct ID, which will we take as representative (for their Sex, Height, Weight, etc).

## Simple analysis

Now we are becoming familiar with the dataset, we might like to see what insights we can gain. For example, let's see how many men and women have competed.

```julia
count(==("M"), athlete_df.Sex)  # `==(x)` creates a function `y -> y == x`
```

```julia
count(==("F"), athlete_df.Sex)
```

The `count` function is useful, but this gets tiresome for many "groups" of data. The [SplitApplyCombine.jl](https://github.com/JuliaData/SplitApplyCombine.jl) package contains useful functions for grouping data.

```julia
using SplitApplyCombine

groupcount(athlete_df.Sex)
```

Which allows us to explore larger sets of groupings:

```julia
# How many athletes by country
team_sizes = groupcount(athlete_df.Team)
```
One of the most powerful tricks in data analysis is to sort your data.

```julia
sort(team_sizes)
```

Note that `groupcount` is a specialization of the more flexible `group(keys, values)` function:

```julia
group(athlete_df.Team, athlete_df.Name)
```

Rather than acting at the level of vectors, you can group an entire `DataFrame` into a `GroupedDataFrame` with the `groupby` function provided by *DataFrames.jl*:

```julia
gdf = groupby(athlete_df, :Team)
```

You can apply an operation to each group and bring the data together into a single `DataFrame` via the `combine` function.

```julia
combine(gdf, nrow => :Count)
```

The `nrow => :Count` syntax is a convenience provided by *Dataframes.jl*. We'll explain how it works below.

## Manipulating DataFrames

Now go through all the things you can do with dataframes.

## More analysis

Create some plots to analyze some trends

 * Mean athlete height by year.
 * Gender statistics? E.g. most successful women's teams?
 * Country success vs time.
 * A table to join with would be good. Perhaps country statistics? Show correlation between population vs gold medals, GDP vs gold medals, etc.

Emphasis on plots.

## Notes

* QQQQ - Get dataset:
- Not too many rows, not too little.
- 3-5 correlated variables.  
- Categorical
- Missing
- Two datasets for join

* Beyond arrays, dictionaries, and such... but not yet in a DB.
* The dataframes package/typedtables.jl - basic usage:
 - Filtering rows.
 - Accessing.
 - Modifying.
 - Copy, DeepCopy, etc....
* Splitting, apply, and `by()`
* Join... 
* Missing values and basic imputation.

* EDA (Exploratory data analysis) StatsPlots.jl

# Memory management

* More on the heap vs. stack. 
* The garbage collector.
* Out-of-core example - working/fetching files.... (DataFrames works with "arrow format" (https://en.wikipedia.org/wiki/Apache_Arrow). Example that implements line by line CSV... and then discuss modern paradigms such as Arrow

# ML Datasets

There are some datasets used in machine learning learning, exploration, and (sometimes) practice that are very popular. This [YouTube video](https://www.youtube.com/watch?v=jrtxL5JHpTk) overviews a few such datasets. 

One of the most common datasets is the [MNIST Digits dataset](https://en.wikipedia.org/wiki/MNIST_database). It is composed of monochrome digits of $28 \times 28$ pixels and **labels** indicating each digit. There are $60,000$ digits that are called the **training set** and $10,000$ additional digits that are called the **test set**. In Julia you can access this dataset via the `MLDatasets.jl` package.


```julia
using MLDatasets
train_data = MLDatasets.MNIST.traindata(Float64)

imgs = train_data[1]
@show typeof(imgs)
@show size(imgs)

labels = train_data[2]
@show typeof(labels);
```

```julia
test_data = MLDatasets.MNIST.testdata(Float64)
test_imgs = test_data[1]
test_labels = test_data[2]
@show size(test_imgs);
```

```julia
n_train, n_test = length(labels), length(test_labels)
```

```julia
using Plots, Measures, LaTeXStrings
println("The first 12 digits: ", labels[1:12])
plot([heatmap(train_data[1][:,:,k]',
            yflip=true,legend=false,c=cgrad([:black, :white])) for k in 1:12]...)
```
It is sometimes useful to "compact" each image into a vector. This then yields a matrix of $60,000 \times 784$ (each image has $784$ features since $784 = 28 \times 28$. We can plot that matrix as well.

```julia
X = vcat([vec(imgs[:,:,k])' for k in 1:last(size(imgs))]...)
@show size(X)
heatmap(X,legend=false)
```

# Unsupervised learning

In general, **unsupervised learning** is the process of learning attributes of the data based only on relationships between features (pixels in the the context of images) and not based on data labels $Y$. So in unsupervised learning we only have $X$.

One type of unsupervised learning focusing on **data reduction** is **principal component analysis (PCA)**. Here the data is projected from a 784 dimensional space into a smaller dimension of our choice. So for example if we choose the smaller dimension to be $2$ then we can "plot" each data point on this new plane. Doing so and comparing some of the digits we can get a feel for how PCA can be useful.

```julia
using MultivariateStats
pca = fit(PCA, X'; maxoutdim=2)
M = projection(pca)

args = (ms=0.8, msw=0, xlims=(-5,12.5), ylims=(-7.5,7.5),
            legend = :topright, xlabel="PC 1", ylabel="PC 2")

function compareDigits(dA,dB)
    xA = X[labels .== dA, :]'
    xB = X[labels .== dB, :]'
    zA, zB = M'*xA, M'*xB
    
    scatter(zA[1,:], zA[2,:], c=:red, label="Digit $(dA)"; args...)
    scatter!(zB[1,:], zB[2,:], c=:blue, label="Digit $(dB)"; args...)
end

plots = []
for k in 1:5
    push!(plots,compareDigits(2k-2,2k-1))
end
plot(plots...,size = (800, 500), margin = 5mm)
```

Another common form of unsupervised learning is **clustering**. The goal of clustering is to find data samples that are homogenous and group them together. Within such group (or cluster) you can also find a representative data point or an average of data points in the group. This is sometimes called a **centroid** or **centre**. The most basic clustering algorithm is $k$-means. It operates by iterating the search of the centres of clusters and the data points that are in the cluster. The parameter $k$ is given before hand as the number of clusters. 

Here we use the `Clustering.jl` package to apply $k$-means to MNIST. We then plot the resulting centroids. 

```julia
using Clustering, Random
Random.seed!(0)
clusterResult = kmeans(X',10)
heatmap(hcat([reshape(clusterResult.centers[:,k],28,28)' for k in 1:10]...),
    yflip=true,legend=false,aspectratio = 1,ticks=false,c=cgrad([:black, :white]))
```

# Supervised learning

A much more common form of machine learning is **supervised learning** where the data is comprised of the features (or independent variables) $X$ and the labels (or dependent variables or response variables) $Y$. 

In cases where the variable to be predicted, $Y$, is a continuos or numerical variable, the problem is typically called a **regression problem**. Otherwise, in cases where $Y$ comes from some finite set of label values the problem is called a **classification problem**. For example in the case of MNIST digits, it is a classification problem since we need to look at an image and determine the respective digit.

## A taste of deep learning

Before we deal with a few basic introductory methods, let us get a glimpse of methods that are near the current "state of the art". These are generally **deep learning** methods which are based on multi-layer (deep) neural networks. See for example the material in the course, [The Mathematical Engineering of Deep Learning](https://deeplearningmath.org/index.html) as well as linked material from there.

To demonstrate we use the package `Metalhead.jl` which supplies "out of the box" deep learning models ready to use. The VGG deep convolutional neural network architecture is one classic model. 

This code downloads the pre-trained VGG19 model which is about 0.5Gb (!!!) of data. It then prints the layers of this deep neural network.

```julia
using Metalhead

#downloads about 0.5Gb of a pretrained neural network from the web
vgg = VGG19();
vgg = VGG19();
for (i,layer) in enumerate(vgg.layers)
    println(i,":\t", layer)
end
```

We don't cover the meaning of such a model in this course. See for example [this AMSI summer school course](https://deeplearningmath.org/) or the [UQ course on deep learning](https://www.nan-ye.com/teach/stat4402/). But we can use this pretrained model.

```julia
#download an arbitrary image and try to classify it
download("https://deeplearningmath.org/data/images/appleFruit.jpg","appleFruit.jpg");
img = load("appleFruit.jpg")
```

```julia
classify(vgg,img)
```

The image is actually not of "Granny Smith" apple, but rather a different type of apple. Still it is pretty close! VGG19 was trained on the famous [Image net database](https://www.image-net.org/).

## The basics: linear models

We don't have time in this course to get into the full details of deep learning. But we can start with a (degenerate) neural network - a linear model. 

With MNIST we can reach $86\%$ classification accuracy with a linear model. This is by no means the 99%+ accuracy that you can obtain with neural network models, but it is still an impressive measures for such a simple model. In discussing this we also explore basic aspects of classification problems.

Our data is comprised of images $x^{(1)},\ldots,x^{(n)}$ where we treat each image as $x^{(i)} \in {\mathbb R}^{784}$. There are then labels $y^{(1)},\ldots,y^{(n)}$ where $y^{(i)} \in \{0,1,2,\ldots,9\}$. For us $n=60,000$.

One approach for such **multi-class classification** (more than just **binary classification**) is to break the problem up into $10$ different binary classification problems. In each of the $10$ cases we train a classifier to classify if a digit is $0$ or not, $1$ or not, $2$ or not, etc... 

If for example we train a classifier to detect $0$ or not, we set a new set of labels $\tilde{y}^{(i)}$ as $+1$ if $y^{(i)}$ is a $0$ and $-1$ if $y^{(i)}$ is not $0$. We can call this a **positive sample** and **negative sample** respectively. 

To predict if a sample is positive or negative we use the basic linear model,

$$
\hat{y}^{(i)} = \beta_0 + \sum_{j=1}^{784} \beta_j x_j^{(i)}. 
$$

To find a "good"  $\beta \in {\mathbb R}^{785}$, our aim is to minimize the **quadratic loss** $\big(\hat{y}^{(i)} - \tilde{y}^{(i)}\big)^2$. Summing over all observations, we have the **loss function**:

$$
L(\beta) = \sum_{i=1}^n \big( [1 ~~ {x^{(i)}}^\top ]^\top  ~ \beta - \tilde{y}^{(i)} \big)^2.
$$

Now we can define the $n\times p$ where $p=785$ **design matrix** which has a first column of $1$'s and the remaining columns each corresponding individual features (or pixels). 

$$
A=\left[\begin{array}{ccccc}
1 & x_{1}^{(1)} & x_{2}^{(1)} & \cdots & x_{p}^{(1)} \\
1 & x_{1}^{(2)} & x_{2}^{(2)} & \cdots & x_{p}^{(2)} \\
\vdots & \vdots & \vdots & & \vdots \\
1 & x_{1}^{(n)} & x_{2}^{(n)} & \cdots & x_{p}^{(n)}
\end{array}\right]
$$

```julia
A = [ones(n_train) X];
```

With the matrix $A$ at hand the loss can be represented as,

$$
L(\beta) = ||A \beta - \tilde{y}||^2 = (A \beta - \tilde{y})^\top (A \beta - \tilde{y}).
$$

The gradient of $L(\beta)$ is 

$$
\nabla L(\beta) = 2 A^{\top}(A \beta - \tilde{y}).
$$

We could use it for **gradient descent** (as we illustrate below) or directly equating to the $0$ vector we have the **normal equations**:

$$
A^\top A \beta = A^\top \tilde{y}.
$$

These equations are solved via 
$$
\beta = A^\dagger \tilde{y},
$$

where $A^\dagger$ is the [Moore-Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse), which can be represented as $A^\dagger = (A^\top A)^{-1} A^\top$ if $A$ is skinny and full rank, but can also be computed in other cases. In any case, using the `LinearAlgebra` package we may obtain it via `pinv()`:

```julia
using LinearAlgebra
Adag = pinv(A);
@show size(Adag);
```

See also Unit 3 where we illustrated solutions of such least squares problems using multiple linear algebraic methods.

We can now find $\beta$ coefficients matching each digit. Interestingly we only need to compute the pseudo-inverse once. This is done in the code below using `onehotbatch()` supplied via the `Flux.jl` package. This function simply converts a label (0-9) to a unit vector. 

```julia
using Flux: onehotbatch

tfPM(x) = x ? +1 : -1
yDat(k) = tfPM.(onehotbatch(labels,0:9)'[:,k+1])
bets = [Adag*yDat(k) for k in 0:9]; #this is the trained model (a list of 10 beta coeff vectors)
```

A binary classifier to then decide if a digit is say $0$ or not would be based on the sign (positive or negative) of the inner product of the corresponding coefficient vector in `bets` and $[1 ~~ {x^{(i)}}^\top ]$. If the inner product is positive the classifier would conclude it is a $0$ digit, otherwise not.

To convert such binary classifier into a multi-class classifier via a **one-vs-rest** approach we simply try the inner products with all $10$ coefficient vectors and choose the digit that maximizes this inner product. Here is the actual function we would use.

```julia
linear_classify(square_image) = argmax([([1 ; vec(square_image)])'*bets[k] for k in 1:10])-1
```

We can now try `linear_classify()` on the 10,000 test images. In addition to computing the accuracy, we also print the **confusion matrix**:

```julia
predictions = [linear_classify(test_imgs[:,:,k]) for k in 1:n_test]
confusionMatrix = [sum((predictions .== i) .& (test_labels .== j)) for i in 0:9, j in 0:9]
acc = sum(diag(confusionMatrix))/n_test

println("Accuracy: ", acc, "\nConfusion Matrix:")
show(stdout, "text/plain", confusionMatrix)
```

Note that with unbalanced data, accuracy is a poor performance measure. However MNIST digits are roughly balanced.

## Getting a feel for gradient based learning

The type of model training we used above relied on an explicit characterization of minimizers of the loss function: solutions of the normal equations. However in more general settings such as deep neural networks, or even in the linear setting with higher dimensions, explicit solution of the first order conditions for minimization (normal equations in the case of linear models) is not possible. For this we often use **iterative optimization** of which the most basic form is based on **first order methods** that move in the direction of local best descent.

We already saw a simple example of **gradient descent** in Unit 3. This can work fine for small or moderate problems but in cases where there is huge data, multiple local minima, or other problems, one often uses **stochastic gradient descent**(SGD) or variants. As a simple illustration, the example below (approximately) optimizes a simple linear regression problem with SGD. Note that another very common which we don't show here is the use of **mini-batches**.

```julia
using Random, Distributions
Random.seed!(1)

n = 10^3
beta0, beta1, sigma = 2.0, 1.5, 2.5
eta = 10^-3

xVals = rand(0:0.01:5,n)
yVals = beta0 .+ beta1*xVals + rand(Normal(0,sigma),n)

pts, b = [], [0, 0]
push!(pts,b)
for k in 1:10^4
    i = rand(1:n)
    g = [   2(b[1] + b[2]*xVals[i]-yVals[i]),
            2*xVals[i]*(b[1] + b[2]*xVals[i]-yVals[i])  ]
    global b -= eta*g
    push!(pts,b)
end

p1 = plot(first.(pts),last.(pts), c=:black,lw=0.5,label="SGD path")
     scatter!([b[1]],[b[2]],c=:blue,ms=5,label="SGD")
     scatter!([beta0],[beta1],
        c=:red,ms=5,label="Actual",
        xlabel=L"\beta_0", ylabel=L"\beta_1",
        ratio=:equal, xlims=(0,2.5), ylims=(0,2.5))

p2 = scatter(xVals,yVals, c=:black, ms=1, label="Data points")
     plot!([0,5],[b[1],b[1]+5b[2]], c=:blue,label="SGD")
     plot!([0,5],[beta0,beta0+5*beta1],	c=:red, label="Actual",
            xlims=(0,5), ylims=(-5,15), xlabel = "x", ylabel = "y")

plot(p1, p2, legend=:topleft, size=(800, 400), margin = 5mm)
```


# Tree based models 

* Hand made decision tree on simple dataset.
* Decision trees
* Boosting 
* Random forests

# Tips for implementing random forests.


